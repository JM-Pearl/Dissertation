{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "810f1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm\n",
    "# import boto3\n",
    "# client = boto3.client('s3')\n",
    "\n",
    "# Code from https://github.com/derekgreene/dynamic-nmf\n",
    "%run Greene_dnmf.py\n",
    "\n",
    "# get procedural stop words\n",
    "%run procedural_stop_words.py\n",
    "\n",
    "from plotnine import ggplot, aes, geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23d26f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Results/Official_TopicModel_80k.pkl','rb') as File:\n",
    "    models = joblib.load(File)\n",
    "    \n",
    "all_df = pd.read_csv('Results/All_speeches_labelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b6ff186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_rankings(H,terms,ntop):\n",
    "    term_rankings = []\n",
    "    for topic_index in range(H.shape[0]):\n",
    "        top_indices = np.argsort(H[topic_index,:])[::-1]\n",
    "        term_ranking = [terms[i] for i in top_indices[:ntop]]\n",
    "        term_rankings.append(term_ranking)\n",
    "    return term_rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "721834d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data(year):\n",
    "    sub_df = all_df.loc[all_df.year_x == year]\n",
    "    model = [mod for mod in models['window_models'] if mod['year'] == year][0]\n",
    "    \n",
    "    Y = [1 if i == 'D' else 0 for i in sub_df.party_y]\n",
    "    X = model['W']\n",
    "    \n",
    "    \n",
    "    return X,Y,sub_df[['party_y','speaker']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "992dae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X,Y,df):\n",
    "    _models = []\n",
    "    for C in np.arange(0.1,1,0.1):\n",
    "        mod = LogisticRegression(penalty='l1',solver='liblinear',class_weight='balanced',C=C)\n",
    "        mod.fit(X,Y)\n",
    "        predictions = mod.predict(X)\n",
    "        speech_accuracy = sum([1 for ix,i in enumerate(predictions) if i == Y[ix]])/len(Y)\n",
    "        \n",
    "        speech_probs = [i[1] for i in mod.predict_proba(X)]\n",
    "        df['prob_party'] = speech_probs\n",
    "        \n",
    "        partisan_assigned = df.groupby('speaker').prob_party.mean().reset_index()\n",
    "        partisan_assigned['predicted_party'] = partisan_assigned.prob_party.apply(lambda x: 'D' if x > 0.5 else 'R')\n",
    "        correct_speaker = sum(df.groupby('speaker').party_y.first().reset_index()\n",
    "                             .merge(partisan_assigned,on='speaker',how='inner')\n",
    "                             .apply(lambda x: 1 if x.party_y == x.predicted_party else 0,1))\n",
    "        \n",
    "        speaker_accuracy = correct_speaker/len(partisan_assigned)\n",
    "        _models.append({\"C\":C,'speech_accuracy':speech_accuracy,'speaker_accuracy':speaker_accuracy,'coefs':mod.coef_})\n",
    "    return pd.DataFrame(_models), partisan_assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d8f81334",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:43<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "speaker_unsumarized = []\n",
    "for year in tqdm(range(1983,2017)):\n",
    "    X,Y,df = select_data(year)\n",
    "    mod_df,var_df = fit_model(X,Y,df)\n",
    "    records.append({\"year\":year,\n",
    "                    \"speaker_accuracy\":mod_df.speaker_accuracy.max(),\n",
    "                    \"speaker_acc_C\":mod_df.loc[mod_df.speaker_accuracy == mod_df.speaker_accuracy.max(),'C'].values[0],\n",
    "                    \"speaker_acc_coef\":mod_df.loc[mod_df.speaker_accuracy == mod_df.speaker_accuracy.max(),'coefs'].values[0],\n",
    "                    \"speech_accuracy\":mod_df.speech_accuracy.max(),\n",
    "                   \"speech_acc_C\":mod_df.loc[mod_df.speech_accuracy == mod_df.speech_accuracy.max(),'C'].values[0],\n",
    "                   \"speech_acc_coef\":mod_df.loc[mod_df.speech_accuracy == mod_df.speech_accuracy.max(),'coefs'].values[0]})\n",
    "    var_df = var_df.merge(df.groupby('speaker').first(),on='speaker',how='left')\n",
    "    var_df['year'] = year\n",
    "    speaker_unsumarized.append(var_df)\n",
    "true_records = pd.DataFrame(records)\n",
    "speaker_all = pd.concat(speaker_unsumarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f06be2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_all.to_csv('Results/speaker_estimates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "19ceb78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_records.to_csv('Results/classification_results_with_coefs.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ad23b",
   "metadata": {},
   "source": [
    "### Chance models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_C = true_records.loc[true_records.year == year,'speaker_acc_C'].values[0]\n",
    "speech_C = true_records.loc[true_records.year == year,'speech_acc_C'].values[0]\n",
    "for i in tqdm(range(200),desc=f\"{year}: \"): # bootstrap sample\n",
    "    boot_df = df.sample(len(df),replace=True)\n",
    "    boot_x = X[boot_df.index]\n",
    "    boot_y = [Y[_] for _ in boot_df.index]\n",
    "\n",
    "    # Model for speaker accuracy\n",
    "    mod = LogisticRegression(penalty='l1',solver='liblinear',class_weight='balanced',C=speaker_C)\n",
    "    mod.fit(boot_x,boot_y)\n",
    "\n",
    "    speech_probs = [i[1] for i in mod.predict_proba(boot_x)]\n",
    "    boot_df['prob_party'] = speech_probs\n",
    "\n",
    "    partisan_assigned = boot_df.groupby('speaker').prob_party.mean().reset_index()\n",
    "    partisan_assigned['predicted_party'] = partisan_assigned.prob_party.apply(lambda x: 'D' if x > 0.5 else 'R')\n",
    "    correct_speaker = sum(df.groupby('speaker').party_y.first().reset_index()\n",
    "                         .merge(partisan_assigned,on='speaker',how='inner')\n",
    "                         .apply(lambda x: 1 if x.party_y == x.predicted_party else 0,1))\n",
    "\n",
    "    speaker_accuracy = correct_speaker/len(partisan_assigned)\n",
    "\n",
    "    # Model for Speech accuracy\n",
    "    mod = LogisticRegression(penalty='l1',solver='liblinear',class_weight='balanced',C=speech_C)\n",
    "    mod.fit(boot_x,boot_y)\n",
    "    predictions = mod.predict(boot_x)\n",
    "    speech_accuracy = sum([1 for ix,i in enumerate(predictions) if i == boot_y[ix]])/len(boot_y)\n",
    "\n",
    "    records.append({\"year\":year,\n",
    "                \"speaker_accuracy\":speaker_accuracy,\n",
    "                \"speech_accuracy\":speech_accuracy,\n",
    "                \"iteration\":i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba15ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1983: 100%|██████████| 200/200 [00:39<00:00,  5.12it/s]\n",
      "1984: 100%|██████████| 200/200 [00:35<00:00,  5.56it/s]\n",
      "1985: 100%|██████████| 200/200 [00:46<00:00,  4.29it/s]\n",
      "1986: 100%|██████████| 200/200 [00:37<00:00,  5.38it/s]\n",
      "1987: 100%|██████████| 200/200 [00:40<00:00,  4.99it/s]\n",
      "1988: 100%|██████████| 200/200 [00:37<00:00,  5.37it/s]\n",
      "1989: 100%|██████████| 200/200 [00:34<00:00,  5.88it/s]\n",
      "1990: 100%|██████████| 200/200 [00:40<00:00,  4.88it/s]\n",
      "1991: 100%|██████████| 200/200 [00:36<00:00,  5.49it/s]\n",
      "1992: 100%|██████████| 200/200 [00:32<00:00,  6.09it/s]\n",
      "1993: 100%|██████████| 200/200 [00:36<00:00,  5.54it/s]\n",
      "1994: 100%|██████████| 200/200 [00:37<00:00,  5.36it/s]\n",
      "1995: 100%|██████████| 200/200 [00:54<00:00,  3.69it/s]\n",
      "1996: 100%|██████████| 200/200 [00:33<00:00,  5.90it/s]\n",
      "1997: 100%|██████████| 200/200 [00:39<00:00,  5.09it/s]\n",
      "1998: 100%|██████████| 200/200 [00:39<00:00,  5.09it/s]\n",
      "1999: 100%|██████████| 200/200 [00:38<00:00,  5.13it/s]\n",
      "2000: 100%|██████████| 200/200 [00:36<00:00,  5.55it/s]\n",
      "2001: 100%|██████████| 200/200 [00:34<00:00,  5.73it/s]\n",
      "2002: 100%|██████████| 200/200 [00:30<00:00,  6.58it/s]\n",
      "2003: 100%|██████████| 200/200 [00:35<00:00,  5.63it/s]\n",
      "2004: 100%|██████████| 200/200 [00:35<00:00,  5.56it/s]\n",
      "2005: 100%|██████████| 200/200 [00:34<00:00,  5.79it/s]\n",
      "2006: 100%|██████████| 200/200 [00:35<00:00,  5.60it/s]\n",
      "2007: 100%|██████████| 200/200 [00:46<00:00,  4.29it/s]\n",
      "2008: 100%|██████████| 200/200 [00:33<00:00,  5.96it/s]\n",
      "2009: 100%|██████████| 200/200 [00:33<00:00,  5.95it/s]\n",
      "2010: 100%|██████████| 200/200 [00:35<00:00,  5.71it/s]\n",
      "2011: 100%|██████████| 200/200 [00:37<00:00,  5.34it/s]\n",
      "2012: 100%|██████████| 200/200 [00:28<00:00,  6.91it/s]\n",
      "2013: 100%|██████████| 200/200 [00:33<00:00,  6.05it/s]\n",
      "2014: 100%|██████████| 200/200 [00:26<00:00,  7.59it/s]\n",
      "2015: 100%|██████████| 200/200 [00:32<00:00,  6.20it/s]\n",
      "2016: 100%|██████████| 200/200 [00:23<00:00,  8.46it/s]\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "for year in range(1983,2017):\n",
    "    speaker_C = true_records.loc[true_records.year == year,'speaker_acc_C'].values[0]\n",
    "    speech_C = true_records.loc[true_records.year == year,'speech_acc_C'].values[0]\n",
    "    \n",
    "    X,Y,df = select_data(year)\n",
    "    \n",
    "    for i in tqdm(range(200),desc=f\"{year}: \"): \n",
    "        np.random.shuffle(Y)\n",
    "\n",
    "        # Model for speaker accuracy\n",
    "        mod = LogisticRegression(penalty='l1',solver='liblinear',class_weight='balanced',C=speaker_C)\n",
    "        mod.fit(X,Y)\n",
    "\n",
    "        speech_probs = [i[1] for i in mod.predict_proba(X)]\n",
    "        df['prob_party'] = speech_probs\n",
    "\n",
    "        partisan_assigned = df.groupby('speaker').prob_party.mean().reset_index()\n",
    "        partisan_assigned['predicted_party'] = partisan_assigned.prob_party.apply(lambda x: 'D' if x > 0.5 else 'R')\n",
    "        correct_speaker = sum(df.groupby('speaker').party_y.first().reset_index()\n",
    "                             .merge(partisan_assigned,on='speaker',how='inner')\n",
    "                             .apply(lambda x: 1 if x.party_y == x.predicted_party else 0,1))\n",
    "\n",
    "        speaker_accuracy = correct_speaker/len(partisan_assigned)\n",
    "\n",
    "        # Model for Speech accuracy\n",
    "        mod = LogisticRegression(penalty='l1',solver='liblinear',class_weight='balanced',C=speech_C)\n",
    "        mod.fit(X,Y)\n",
    "        predictions = mod.predict(X)\n",
    "        speech_accuracy = sum([1 for ix,i in enumerate(predictions) if i == Y[ix]])/len(Y)\n",
    "\n",
    "        records.append({\"year\":year,\n",
    "                    \"speaker_accuracy\":speaker_accuracy,\n",
    "                    \"speech_accuracy\":speech_accuracy,\n",
    "                    \"iteration\":i})\n",
    "    \n",
    "null_records = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a603f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = pd.concat([null_records,true_records[['year','speaker_accuracy','speech_accuracy']]])\n",
    "combo.loc[combo.iteration.isnull(),'type']  = 'actual'\n",
    "combo.loc[-combo.iteration.isnull(),'type']  = 'null'\n",
    "combo.loc[combo.iteration.isnull(),'iteration']  = 11\n",
    "combo.to_csv('results_with_null.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cdaad4",
   "metadata": {},
   "source": [
    "### Bootstrapped models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "741f88b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_boot(year):\n",
    "    records = []\n",
    "    X,Y,df = select_data(year)\n",
    "    speaker_C = true_records.loc[true_records.year == year,'speaker_acc_C'].values[0]\n",
    "    speech_C = true_records.loc[true_records.year == year,'speech_acc_C'].values[0]\n",
    "    for i in tqdm(range(1000),desc=f\"{year}: \"): # bootstrap sample\n",
    "        boot_df = df.sample(len(df),replace=True)\n",
    "        boot_x = X[boot_df.index]\n",
    "        boot_y = [Y[_] for _ in boot_df.index]\n",
    "        \n",
    "        # Model for speaker accuracy\n",
    "        mod = LogisticRegression(penalty='l1',solver='liblinear',class_weight='balanced',C=speaker_C)\n",
    "        mod.fit(boot_x,boot_y)\n",
    "        \n",
    "        speech_probs = [i[1] for i in mod.predict_proba(boot_x)]\n",
    "        boot_df['prob_party'] = speech_probs\n",
    "        \n",
    "        partisan_assigned = boot_df.groupby('speaker').prob_party.mean().reset_index()\n",
    "        partisan_assigned['predicted_party'] = partisan_assigned.prob_party.apply(lambda x: 'D' if x > 0.5 else 'R')\n",
    "        correct_speaker = sum(boot_df.groupby('speaker').party_y.first().reset_index()\n",
    "                             .merge(partisan_assigned,on='speaker',how='inner')\n",
    "                             .apply(lambda x: 1 if x.party_y == x.predicted_party else 0,1))\n",
    "\n",
    "        speaker_accuracy = correct_speaker/len(partisan_assigned)\n",
    "    \n",
    "        # Model for Speech accuracy\n",
    "        mod = LogisticRegression(penalty='l1',solver='liblinear',class_weight='balanced',C=speech_C)\n",
    "        mod.fit(boot_x,boot_y)\n",
    "        predictions = mod.predict(boot_x)\n",
    "        speech_accuracy = sum([1 for ix,i in enumerate(predictions) if i == boot_y[ix]])/len(boot_y)\n",
    "        \n",
    "        records.append({\"year\":year,\n",
    "                    \"speaker_accuracy\":speaker_accuracy,\n",
    "                    \"speech_accuracy\":speech_accuracy})\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d0399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1983: 100%|██████████| 1000/1000 [04:26<00:00,  3.76it/s]\n",
      "1984: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
      "1985: 100%|██████████| 1000/1000 [05:12<00:00,  3.20it/s]\n",
      "1986: 100%|██████████| 1000/1000 [04:20<00:00,  3.85it/s]\n",
      "1987: 100%|██████████| 1000/1000 [04:17<00:00,  3.89it/s]\n",
      "1988: 100%|██████████| 1000/1000 [03:54<00:00,  4.26it/s]\n",
      "1989: 100%|██████████| 1000/1000 [03:50<00:00,  4.34it/s]\n",
      "1990: 100%|██████████| 1000/1000 [04:20<00:00,  3.84it/s]\n",
      "1991: 100%|██████████| 1000/1000 [04:33<00:00,  3.66it/s]\n",
      "1992: 100%|██████████| 1000/1000 [03:39<00:00,  4.56it/s]\n",
      "1993: 100%|██████████| 1000/1000 [04:50<00:00,  3.44it/s]\n",
      "2006:  76%|███████▌  | 762/1000 [03:30<01:03,  3.75it/s]"
     ]
    }
   ],
   "source": [
    "boot_results = []\n",
    "for year in range(1983,2017):\n",
    "    boot_results.append(Run_boot(year))\n",
    "boot_df = pd.concat(boot_results)\n",
    "boot_df.to_csv('bootstrap_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a6d7e",
   "metadata": {},
   "source": [
    "## With SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c02e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X,Y,df):\n",
    "    _models = []\n",
    "    for C in [0.1, 1, 10, 100, 1000]:\n",
    "        mod = SVC(kernel='linear',class_weight='balanced',C=C,probability=True)\n",
    "        mod.fit(X,Y)\n",
    "        predictions = mod.predict(X)\n",
    "        speech_accuracy = sum([1 for ix,i in enumerate(predictions) if i == Y[ix]])/len(Y)\n",
    "        \n",
    "        speech_probs = [i[1] for i in mod.predict_proba(X)]\n",
    "        df['prob_party'] = speech_probs\n",
    "        \n",
    "        partisan_assigned = df.groupby('speaker').prob_party.mean().reset_index()\n",
    "        partisan_assigned['predicted_party'] = partisan_assigned.prob_party.apply(lambda x: 'D' if x > 0.5 else 'R')\n",
    "        correct_speaker = sum(df.groupby('speaker').party_y.first().reset_index()\n",
    "                             .merge(partisan_assigned,on='speaker',how='inner')\n",
    "                             .apply(lambda x: 1 if x.party_y == x.predicted_party else 0,1))\n",
    "        \n",
    "        speaker_accuracy = correct_speaker/len(partisan_assigned)\n",
    "        _models.append({\"C\":C,'speech_accuracy':speech_accuracy,'speaker_accuracy':speaker_accuracy,'coefs':mod.coef_})\n",
    "    return pd.DataFrame(_models), partisan_assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eede683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 10/34 [2:03:41<4:29:02, 672.59s/it]"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "speaker_unsumarized = []\n",
    "for year in tqdm(range(1983,2017)):\n",
    "    X,Y,df = select_data(year)\n",
    "    mod_df,var_df = fit_model(X,Y,df)\n",
    "    records.append({\"year\":year,\n",
    "                    \"speaker_accuracy\":mod_df.speaker_accuracy.max(),\n",
    "                    \"speaker_acc_C\":mod_df.loc[mod_df.speaker_accuracy == mod_df.speaker_accuracy.max(),'C'].values[0],\n",
    "                    \"speaker_acc_coef\":mod_df.loc[mod_df.speaker_accuracy == mod_df.speaker_accuracy.max(),'coefs'].values[0],\n",
    "                    \"speech_accuracy\":mod_df.speech_accuracy.max(),\n",
    "                   \"speech_acc_C\":mod_df.loc[mod_df.speech_accuracy == mod_df.speech_accuracy.max(),'C'].values[0],\n",
    "                   \"speech_acc_coef\":mod_df.loc[mod_df.speech_accuracy == mod_df.speech_accuracy.max(),'coefs'].values[0]})\n",
    "    var_df = var_df.merge(df.groupby('speaker').first(),on='speaker',how='left')\n",
    "    var_df['year'] = year\n",
    "    speaker_unsumarized.append(var_df)\n",
    "true_records = pd.DataFrame(records)\n",
    "speaker_all = pd.concat(speaker_unsumarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_records.to_csv('Results/SVC_classification_results_with_coefs.csv')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
