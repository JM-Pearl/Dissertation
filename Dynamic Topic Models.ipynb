{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c611239",
   "metadata": {},
   "source": [
    "# Dynamic Topic Model\n",
    "\n",
    "This notebook includes code for creating window topic models for each year of congressional speech in the House for each party, and a dynamic topic model as per the Dynamic Non-Negative Matrix Factorization approach described by Greene (2019). \n",
    "\n",
    "\n",
    "Previous development of these models indicates that the most interpretable and coherent models generally fall between 45 and 60 topics for each year/party respectively. Little difference is made in the interpretability and coherence of models within this range. For this reason a middleground of 50 topics is used for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e73ac61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from multiprocess import Pool\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import boto3\n",
    "client = boto3.client('s3')\n",
    "\n",
    "# Code from https://github.com/derekgreene/dynamic-nmf\n",
    "%run Greene_dnmf.py\n",
    "\n",
    "# get procedural stop words\n",
    "%run procedural_stop_words.py\n",
    "\n",
    "# model evaluation tool \n",
    "%run Model_description_evaluation_widget.py\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c99b4147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_rankings(H,terms,ntop):\n",
    "    term_rankings = []\n",
    "    for topic_index in range(H.shape[0]):\n",
    "        top_indices = np.argsort(H[topic_index,:])[::-1]\n",
    "        term_ranking = [terms[i] for i in top_indices[:ntop]]\n",
    "        term_rankings.append(term_ranking)\n",
    "    return term_rankings\n",
    "\n",
    "def get_top_words(vect):\n",
    "    splits = [[z for z in i.split() if z in vocab] for i in vect]\n",
    "    docs = [x for sublist in splits for x in sublist]\n",
    "    counts = Counter(docs)\n",
    "    top_10 = [i[0] for i in counts.most_common()][:20]\n",
    "    return top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d705614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_window_NMF(congress):\n",
    "    \n",
    "    \"\"\"\n",
    "    Reads in data for a given congress, runs NMF models for every year \n",
    "    in that congress for a specific party.\n",
    "    \n",
    "    congress: what congress to use\n",
    "    \"\"\"\n",
    "\n",
    "    k = 65\n",
    "    \n",
    "    # read in data from S3\n",
    "    DF = pd.read_csv(client.get_object(Bucket='ascsagemaker',\n",
    "                                       Key=f'JMP_congressional_nmf/House_bigrams/{congress:0>3}_fixed_party.csv')['Body'])\n",
    "    \n",
    "    # remove speeches with no party labels\n",
    "    DF = DF.loc[-DF.party_y.isnull()]\n",
    "\n",
    "    # partse to only the house and party of interest\n",
    "    DF = DF.loc[(DF.chamber_x == 'H')]\n",
    "\n",
    "    DF['date'] = pd.to_datetime(DF.date)  # to date time\n",
    "    \n",
    "    years = pd.to_datetime(DF.date).dt.year.unique() # what years are included in this congress\n",
    "    if congress == 112:  #  112th congress includes overlap year with 113th\n",
    "        years = years[:2]\n",
    "    models = []\n",
    "    \n",
    "    #  for each year run a NMF window topic model\n",
    "    for year in years:\n",
    "        sub_df = DF.loc[DF.date.dt.year == year]\n",
    "\n",
    "        # prepare TfIDF DTM\n",
    "        vectorizer = TfidfVectorizer(min_df=0.001,max_df=0.30,stop_words=procedural_stop_words,use_idf=True,)\n",
    "        dtm = vectorizer.fit_transform(sub_df.speech_processed)\n",
    "        vocab = vectorizer.get_feature_names()\n",
    "\n",
    "        # run model\n",
    "        model = NMF(n_components=k,max_iter=5000,init='nndsvd')\n",
    "        W = model.fit_transform(dtm)\n",
    "        H = model.components_\n",
    "        print(f'{year} - {len(sub_df)} speeches')\n",
    "        \n",
    "        # return information packet\n",
    "        models.append({\"W\":W,\"H\":H,\n",
    "                       \"vocab\":vocab,\n",
    "                       \"window_labels\":[f'{year}_{i}' for i in range(100)],\n",
    "                       \"year\":year,\n",
    "                       \"DF_index\":DF.index,\n",
    "                       \"topics\":term_rankings(H,vocab,ntop=10)})\n",
    "        \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae909a03",
   "metadata": {},
   "source": [
    "## Run batch process of every year (34 models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cd3fe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1997 - 18073 speeches\n",
      "1999 - 18494 speeches\n",
      "2001 - 15038 speeches\n",
      "1985 - 21288 speeches\n",
      "1983 - 19450 speeches\n",
      "1991 - 18944 speeches\n",
      "1995 - 30208 speeches\n",
      "1989 - 14525 speeches\n",
      "1993 - 18404 speeches\n",
      "2002 - 12114 speeches\n",
      "1998 - 18098 speeches\n",
      "1987 - 19235 speeches\n",
      "1992 - 15767 speeches\n",
      "2003 - 17069 speeches\n",
      "1984 - 20144 speeches\n",
      "1986 - 17440 speeches\n",
      "2000 - 17704 speeches\n",
      "1990 - 18562 speeches\n",
      "1996 - 17943 speeches\n",
      "2004 - 14759 speeches\n",
      "2013 - 13921 speeches\n",
      "2005 - 17568 speeches\n",
      "1994 - 16559 speeches\n",
      "2009 - 14239 speeches\n",
      "1988 - 16095 speeches\n",
      "2015 - 14138 speeches\n",
      "2006 - 15252 speeches\n",
      "2016 - 8806 speeches\n",
      "2014 - 11670 speeches\n",
      "2010 - 15235 speeches\n",
      "2011 - 17205 speeches\n",
      "2012 - 12306 speeches\n",
      "2007 - 24529 speeches\n",
      "2008 - 14826 speeches\n"
     ]
    }
   ],
   "source": [
    "with Pool(10) as p:\n",
    "    output = p.map(run_window_NMF,range(98,115))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43415987",
   "metadata": {},
   "source": [
    "## Prepare window NMF for dynamic level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db2f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [model for sublist in output for model in sublist]\n",
    "\n",
    "collection = TopicCollection()\n",
    "for model in outputs:\n",
    "    collection.add_topic_model(model['H'],model['vocab'],model['window_labels'])\n",
    "    \n",
    "Mat, full_vocab = collection.create_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa69ad9",
   "metadata": {},
   "source": [
    "## Run Dynamic Model\n",
    "\n",
    "After testing many different window K and dynamic K the following were found. After a window K of 50, there is little difference in the number of substantive topics identified. After windowK of 70 topics seem to start repeating more than twice. a window topic K of 65 was thus chosen for all window models. \n",
    "\n",
    "Dynamic K was selected by testing a range between 65 and 100 by increments of 5. Meaningful separation of topics was found at 95 topics. As such this number, 95, was selected for the final model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b0e31f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level = NMF(n_components=160,max_iter=5000,init='nndsvd')\n",
    "W = second_level.fit_transform(Mat)\n",
    "H = second_level.components_\n",
    "terms = term_rankings(H,full_vocab,ntop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e79846f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6636573aab4e48bf8685defa1284e897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Dropdown(description='Dynamic Topic #:', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "term_list = [term for sublist in outputs for term in sublist['topics']]\n",
    "mapped_df = pd.DataFrame({\"window_descriptions\":term_list,\n",
    "                          'window_id':[' - '.join(i.split('_')) for i in collection.topic_ids],\n",
    "                          'dynamic_id':W.argmax(1)})\n",
    "\n",
    "\n",
    "box = run_widget(terms,mapped_df)\n",
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d4b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bd0f703",
   "metadata": {},
   "source": [
    "### DataFrame to map window topics to dynamic topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6c79e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_topics = W.argmax(1)\n",
    "assigned_labels = [dynamic_labels[i] for i in assigned_topics]\n",
    "mapper = []\n",
    "for i in range(len(collection.topic_ids)):\n",
    "    year,topic = collection.topic_ids[i].split('_')\n",
    "    topic_label = assigned_labels[i]\n",
    "    topic_ix = assigned_topics[i]\n",
    "    mapper.append({'year':year,'topic_id':int(topic),'dynamic_label':topic_label})\n",
    "\n",
    "mapper = pd.DataFrame(mapper)\n",
    "\n",
    "def get_topic_terms(x):\n",
    "    year = int(x['year'])\n",
    "    ix = int(x['topic_id'])\n",
    "    for out in outputs:\n",
    "        if out['year'] == year:\n",
    "            return out['topics'][ix]\n",
    "        \n",
    "        \n",
    "mapper['window_terms'] = mapper.apply(get_topic_terms,1)\n",
    "mapper = mapper.sort_values(by='year',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e5baa7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Final Model/models\n",
    "Final_output = {\"window_models\":outputs,\n",
    "                'dynamic_model':{'k':95,'H':H,'W':W,'collection_mat':Mat,'collection_vocab':full_vocab,'terms':terms},\n",
    "                'mapper':mapper}\n",
    "\n",
    "with open('Official_TopicModel_95k.pkl','wb') as File:\n",
    "    joblib.dump(Final_output,File)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da754c1",
   "metadata": {},
   "source": [
    "## Assign labels to speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "254b4225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back in all the data\n",
    "dfs = []\n",
    "for congress in range(98,115): \n",
    "\n",
    "    DF2 = pd.read_csv(client.get_object(Bucket='ascsagemaker',\n",
    "                                           Key=f'JMP_congressional_nmf/House_bigrams/{congress:0>3}_fixed_party.csv')['Body'])\n",
    "    DF2['date'] = pd.to_datetime(DF2['date'])\n",
    "    \n",
    "    if congress == 112:\n",
    "        DF2 = DF2.loc[DF2.date.dt.year != 2013]\n",
    "        \n",
    "    dfs.append(DF2)\n",
    "    \n",
    "ldf = pd.concat(dfs)\n",
    "ldf = ldf.loc[-ldf.party_y.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2da8c0ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:02<00:00, 12.72it/s]\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None # suppress warning about slicing\n",
    "\n",
    "code_mapper = Final_output['mapper'][['year','topic_id','dynamic_label']]\n",
    "code_mapper['int_year'] = code_mapper.year.apply(lambda x: int(x))\n",
    "\n",
    "labelled_df = []\n",
    "for year in tqdm(ldf.date.dt.year.unique()):\n",
    "    # subset given year and find window W matrix\n",
    "    sub_df = ldf.loc[ldf.date.dt.year == year]\n",
    "    window_model = [model for model in Final_output['window_models'] if model['year'] == year][0]\n",
    "    sub_df.loc[:,'window_topic_id'] = window_model['W'].argmax(1)\n",
    "    # merge on the assigned dynamic topic and year\n",
    "    sub_df = sub_df.merge(code_mapper,left_on=['year','window_topic_id'],right_on=['int_year','topic_id'],how='left')\n",
    "    labelled_df.append(sub_df)\n",
    "    \n",
    "final_DF = pd.concat(labelled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260071e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_DF.to_csv('All_speeches_labelled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d013c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
