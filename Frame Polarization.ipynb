{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7461316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['R_HOME'] = '/home/ec2-user/anaconda3/envs/R/lib/R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c33b19e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%run procedural_stop_words.py\n",
    "procedural_stop_words.extend(['do','be','mr_speaker','have','time','other'])\n",
    "\n",
    "from tqdm import tqdm\n",
    "from plotnine import ggplot, aes, geoms\n",
    "\n",
    "# R package import \n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "R = ro.r\n",
    "pandas2ri.activate()\n",
    "\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import time\n",
    "\n",
    "from multiprocess import Pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75907b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv('Results/All_speeches_labelled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebc896",
   "metadata": {},
   "source": [
    "\n",
    "## For Speakers\n",
    "\n",
    "A massive-univariate technique in which every phrases is tested against the null-hypothesis that the frequency distribution of the word for Democrats and Republicans come from the same underlying distribution. This analysis is akin to the mass-univeraite analysis undertaken in basic neuroimaging research, where each voxel is analyzed independently given the same model. Results are of course corrected for multiple comparison using FDR correction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eae08628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_poisson(col):\n",
    "    \"\"\"\n",
    "    Runs Poisson regression for terms\n",
    "    \"\"\"\n",
    "    \n",
    "    mod = R.glm(f'x_{col}~x_party_y',family='poisson',data=DTM)\n",
    "    effects = R.summary(mod).rx2('coefficients')\n",
    "    return {'col':col,'est':effects[1,0],'pval':effects[1,-1]}\n",
    "\n",
    "\n",
    "def run_model(year,topic,min_df=0.5,num_cpu=30):\n",
    "    \n",
    "    # subset dataframe for year and topic\n",
    "    sub_df = all_df.loc[(all_df.year_y == year) & (all_df.dynamic_label == topic)]\n",
    "    if len(sub_df) > 0:\n",
    "        \n",
    "        # Linker for speaker party to speaker\n",
    "        name_party_link = sub_df[['speaker','party_y']].groupby('speaker').first().reset_index()\n",
    "\n",
    "        # term DTM\n",
    "        vectorizer = CountVectorizer(stop_words=procedural_stop_words,min_df=0.05,binary=True)\n",
    "        DTM = vectorizer.fit_transform(sub_df.speech_processed)\n",
    "        DTM = pd.DataFrame(DTM.toarray())\n",
    "\n",
    "        # sum term occurance by speaker and merge with party\n",
    "        DTM['speaker'] = list(sub_df['speaker'])\n",
    "        DTM = (DTM\n",
    "               .groupby('speaker')\n",
    "               .sum()\n",
    "               .reset_index()\n",
    "               .merge(name_party_link,on='speaker',how='left')\n",
    "               .drop('speaker',1)\n",
    "              )\n",
    "        \n",
    "        # give columns names compatible with R\n",
    "        DTM.columns = [f'x_{i}' for i in DTM.columns]\n",
    "        \n",
    "        # Run Massive Univariate Poisson GLM\n",
    "        with Pool(30) as p:\n",
    "            estimates = p.map(run_poisson,range(len(vectorizer.get_feature_names())))\n",
    "        \n",
    "        # Make to DataFrame\n",
    "        frame = pd.DataFrame(estimates)\n",
    "        frame['term'] = vectorizer.get_feature_names() # add terms\n",
    "        frame['year'] = year\n",
    "        frame['topic'] = topic\n",
    "        frame = frame.drop('col',1)\n",
    "        \n",
    "        # perform FDR correction for multiple comparisons, alpha = 0.05\n",
    "        frame['fdr_p'] = fdrcorrection(df.pval)[0]\n",
    "        \n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frames = []\n",
    "for topic in all_df.dynamic_label.unique(): # for each topic\n",
    "    start_time = time.time()\n",
    "    for year in range(1983,2017): # for every year\n",
    "        Frames.append(run_model(year,topic))    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f'topic - {topic} completed in {(end_time-start_time)/60} minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_terms = pd.concat(Frames)\n",
    "All_terms.to_csv('Results/Univariate_Frame_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f3923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
